#KNN을 배워보자
import numpy as np
import matplotlib.pyplot as plt
from dataset_utils import make_dataset, vis_dataset

#클래스 개수 설정, 랜덤 데이터셋 생성
n_classes = 3
X, y = make _dataset(n_classes=n_classes)
#테스트 데이터 생성
test_data = np.array([1, 1.3])

#K값 설정
K = 5
dist = []
for X_ in X:

#거리는 유클리드 
  dist = np.sqrt(np.sum((test_data - X)**2))
  dists.append(dist)

#거리(차)순으로 정렬하고, 인덱스 값을 반환하는 argsort 
sorted_indices = np.argsort(dists)
#K만큼 선택    
closest_indices = sorted_indices[:K]
#y(정답값, 인덱스, 클래스) 할당
closest_classes = y[closest_indices]

#클래스의 종류 확인 및 갯수 세기 
uniques, cnts = np.unique(closest_classes, return_counts=True)
#argmax를 통해 갯수가 가장 많은 클래스를 산출해서 할당 
pred = uniques[np.argmax(cnts)]

print(uniques)
print(cnts)
print(f"{test_data} is classified as {pred}")

#결과 시각화하기
closest_X = X[closest_indices]

fig, ax = vis_dataset(X, y)
ax.scatter(test_data[0], test_data[1], color='red', s=100,k alpha=0.3)
ax.scatter(test_data[0], test_data[1], s=300, edgecolors=f'{int(pred)}', linewidths=1, alpha=0.3)

for closest_X_, closest_class in zip(closest_X, closest_classes):
  ax.plot([closest_X_[0], test_data[0]],[closest_X_[1], test_data[1]], color=f'C{int(closest_class}', ls=':')

plt.show()







